{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segnet-enet-jaeoh2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUxiENhvRtVYdHyFHcQ8D1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xenagarage/automatic-signal-classification-with-deep-learning/blob/main/segnet_enet_jaeoh2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zw1lI5yldYP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e42b879-9912-44a8-fdec-7947cb28a3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "%tensorflow_version 1.x\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "# from segnet import SegNet\n",
        "# from loss import DiscriminativeLoss\n",
        "# from dataset import tuSimpleDataset\n",
        "# from logger import Logger"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segnet"
      ],
      "metadata": {
        "id": "K4FRW8_Y08BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SEGNET\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class ConvBnRelu(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch, kernel_size=3, padding=1):\n",
        "        super(ConvBnRelu, self).__init__()\n",
        "        self.conv =  nn.Sequential(\n",
        "            nn.Conv2d(input_ch, output_ch, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(output_ch),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "    # refer from : https://github.com/delta-onera/segnet_pytorch/blob/master/segnet.py\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Shared Encoder\n",
        "        self.enc11 = ConvBnRelu(input_ch, 64)\n",
        "        self.enc12 = ConvBnRelu(64, 64)\n",
        "\n",
        "        self.enc21 = ConvBnRelu(64, 128)\n",
        "        self.enc22 = ConvBnRelu(128, 128)\n",
        "\n",
        "        self.enc31 = ConvBnRelu(128, 256)\n",
        "        self.enc32 = ConvBnRelu(256, 256)\n",
        "        self.enc33 = ConvBnRelu(256, 256)\n",
        "\n",
        "        self.enc41 = ConvBnRelu(256, 512)\n",
        "        self.enc42 = ConvBnRelu(512, 512)\n",
        "        self.enc43 = ConvBnRelu(512, 512)\n",
        "\n",
        "        self.enc51 = ConvBnRelu(512, 512)\n",
        "        self.enc52 = ConvBnRelu(512, 512)\n",
        "        self.enc53 = ConvBnRelu(512, 512)\n",
        "\n",
        "        self.init_vgg_weigts()\n",
        "\n",
        "        # Binary Segmentation Decoder\n",
        "        self.sem_dec53 = ConvBnRelu(512, 512)\n",
        "        self.sem_dec52 = ConvBnRelu(512, 512)\n",
        "        self.sem_dec51 = ConvBnRelu(512, 512)\n",
        "\n",
        "        self.sem_dec43 = ConvBnRelu(512, 512)\n",
        "        self.sem_dec42 = ConvBnRelu(512, 512)\n",
        "        self.sem_dec41 = ConvBnRelu(512, 256)\n",
        "\n",
        "        self.sem_dec33 = ConvBnRelu(256, 256)\n",
        "        self.sem_dec32 = ConvBnRelu(256, 256)\n",
        "        self.sem_dec31 = ConvBnRelu(256, 128)\n",
        "\n",
        "        self.sem_dec22 = ConvBnRelu(128, 128)\n",
        "        self.sem_dec21 = ConvBnRelu(128, 64)\n",
        "\n",
        "        self.sem_dec12 = ConvBnRelu(64, 64)\n",
        "\n",
        "        # Instance Segmentation Decoder\n",
        "        self.ins_dec53 = ConvBnRelu(512, 512)\n",
        "        self.ins_dec52 = ConvBnRelu(512, 512)\n",
        "        self.ins_dec51 = ConvBnRelu(512, 512)\n",
        "\n",
        "        self.ins_dec43 = ConvBnRelu(512, 512)\n",
        "        self.ins_dec42 = ConvBnRelu(512, 512)\n",
        "        self.ins_dec41 = ConvBnRelu(512, 256)\n",
        "\n",
        "        self.ins_dec33 = ConvBnRelu(256, 256)\n",
        "        self.ins_dec32 = ConvBnRelu(256, 256)\n",
        "        self.ins_dec31 = ConvBnRelu(256, 128)\n",
        "\n",
        "        self.ins_dec22 = ConvBnRelu(128, 128)\n",
        "        self.ins_dec21 = ConvBnRelu(128, 64)\n",
        "\n",
        "        self.ins_dec12 = ConvBnRelu(64, 64)\n",
        "\n",
        "        self.sem_out = nn.Conv2d(64, output_ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.ins_out = nn.Conv2d(64, 5, kernel_size=3, stride=1, padding=1)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        # Shared Encoder\n",
        "        x = self.enc11(x)\n",
        "        x = self.enc12(x)\n",
        "        x, ind_1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        x = self.enc21(x)\n",
        "        x = self.enc22(x)\n",
        "        x, ind_2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        x = self.enc31(x)\n",
        "        x = self.enc32(x)\n",
        "        x = self.enc33(x)\n",
        "        x, ind_3 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        x = self.enc41(x)\n",
        "        x = self.enc42(x)\n",
        "        x = self.enc43(x)\n",
        "        x, ind_4 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        x = self.enc51(x)\n",
        "        x = self.enc52(x)\n",
        "        x = self.enc53(x)\n",
        "        x, ind_5 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Binary Segmentation Decoder\n",
        "        x1 = F.max_unpool2d(x, ind_5, kernel_size=2, stride=2)\n",
        "        x1 = self.sem_dec53(x1)\n",
        "        x1 = self.sem_dec52(x1)\n",
        "        x1 = self.sem_dec51(x1)\n",
        "\n",
        "        x1 = F.max_unpool2d(x1, ind_4, kernel_size=2, stride=2)\n",
        "        x1 = self.sem_dec43(x1)\n",
        "        x1 = self.sem_dec42(x1)\n",
        "        x1 = self.sem_dec41(x1)\n",
        "\n",
        "        x1 = F.max_unpool2d(x1, ind_3, kernel_size=2, stride=2)\n",
        "        x1 = self.sem_dec33(x1)\n",
        "        x1 = self.sem_dec32(x1)\n",
        "        x1 = self.sem_dec31(x1)\n",
        "\n",
        "        x1 = F.max_unpool2d(x1, ind_2, kernel_size=2, stride=2)\n",
        "        x1 = self.sem_dec22(x1)\n",
        "        x1 = self.sem_dec21(x1)\n",
        "\n",
        "        x1 = F.max_unpool2d(x1, ind_1, kernel_size=2, stride=2)\n",
        "        x1 = self.sem_dec12(x1)\n",
        "\n",
        "        # Instance Segmentation Decoder\n",
        "        x2 = F.max_unpool2d(x, ind_5, kernel_size=2, stride=2)\n",
        "        x2 = self.ins_dec53(x2)\n",
        "        x2 = self.ins_dec52(x2)\n",
        "        x2 = self.ins_dec51(x2)\n",
        "\n",
        "        x2 = F.max_unpool2d(x2, ind_4, kernel_size=2, stride=2)\n",
        "        x2 = self.ins_dec43(x2)\n",
        "        x2 = self.ins_dec42(x2)\n",
        "        x2 = self.ins_dec41(x2)\n",
        "\n",
        "        x2 = F.max_unpool2d(x2, ind_3, kernel_size=2, stride=2)\n",
        "        x2 = self.ins_dec33(x2)\n",
        "        x2 = self.ins_dec32(x2)\n",
        "        x2 = self.ins_dec31(x2)\n",
        "\n",
        "        x2 = F.max_unpool2d(x2, ind_2, kernel_size=2, stride=2)\n",
        "        x2 = self.ins_dec22(x2)\n",
        "        x2 = self.ins_dec21(x2)\n",
        "\n",
        "        x2 = F.max_unpool2d(x2, ind_1, kernel_size=2, stride=2)\n",
        "        x2 = self.ins_dec12(x2)\n",
        "\n",
        "        sem = self.sem_out(x1)\n",
        "        ins = self.ins_out(x2)\n",
        "\n",
        "        return sem, ins\n",
        "\n",
        "    def init_vgg_weigts(self):\n",
        "        self.enc11.conv[0].weight.data = self.vgg16.features[0].weight.data\n",
        "\n",
        "        self.enc12.conv[0].weight.data = self.vgg16.features[2].weight.data\n",
        "\n",
        "        self.enc21.conv[0].weight.data = self.vgg16.features[5].weight.data\n",
        "\n",
        "        self.enc22.conv[0].weight.data = self.vgg16.features[7].weight.data\n",
        "\n",
        "        self.enc31.conv[0].weight.data = self.vgg16.features[10].weight.data\n",
        "\n",
        "        self.enc32.conv[0].weight.data = self.vgg16.features[12].weight.data\n",
        "\n",
        "        self.enc33.conv[0].weight.data = self.vgg16.features[14].weight.data\n",
        "\n",
        "        self.enc41.conv[0].weight.data = self.vgg16.features[17].weight.data\n",
        "\n",
        "        self.enc42.conv[0].weight.data = self.vgg16.features[19].weight.data\n",
        "\n",
        "        self.enc43.conv[0].weight.data = self.vgg16.features[21].weight.data\n",
        "\n",
        "        self.enc51.conv[0].weight.data = self.vgg16.features[21].weight.data\n",
        "\n",
        "        self.enc52.conv[0].weight.data = self.vgg16.features[24].weight.data\n",
        "\n",
        "        self.enc53.conv[0].weight.data = self.vgg16.features[26].weight.data"
      ],
      "metadata": {
        "id": "VFXH8pvvjGc1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enet"
      ],
      "metadata": {
        "id": "EAZ5yVcX1BaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENET\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class InitialBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_ch,\n",
        "                 output_ch,\n",
        "                 bias=False):\n",
        "        super(InitialBlock, self).__init__()\n",
        "\n",
        "        self.main_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_ch,\n",
        "                      out_channels=output_ch - 3,\n",
        "                      kernel_size=3,\n",
        "                      stride=2,\n",
        "                      padding=1,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(output_ch - 3),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.ext_branch = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        main = self.main_branch(x)\n",
        "        ext = self.ext_branch(x)\n",
        "\n",
        "        out = torch.cat((main, ext), dim=1) # N, C, H, W\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class RegularBottleNeck(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_ch,\n",
        "                 output_ch,\n",
        "                 projection_ratio=4,\n",
        "                 regularizer_prob=0,\n",
        "                 dilation=0,\n",
        "                 assymmetric=False,\n",
        "                 bias=False):\n",
        "        super(RegularBottleNeck, self).__init__()\n",
        "\n",
        "        reduced_depth = input_ch // projection_ratio\n",
        "\n",
        "        self.ext_branch_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_ch,\n",
        "                      out_channels=reduced_depth,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(reduced_depth),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        if dilation:\n",
        "            self.ext_branch_2 = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=reduced_depth,\n",
        "                          out_channels=reduced_depth,\n",
        "                          kernel_size=3,\n",
        "                          stride=1,\n",
        "                          padding=dilation,\n",
        "                          bias=bias,\n",
        "                          dilation=dilation),\n",
        "                nn.BatchNorm2d(reduced_depth),\n",
        "                nn.PReLU()\n",
        "            )\n",
        "\n",
        "        elif assymmetric:\n",
        "            self.ext_branch_2 = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=reduced_depth,\n",
        "                          out_channels=reduced_depth,\n",
        "                          kernel_size=(5, 1),\n",
        "                          stride=1,\n",
        "                          padding=(2, 0),\n",
        "                          bias=bias),\n",
        "                nn.Conv2d(in_channels=reduced_depth,\n",
        "                          out_channels=reduced_depth,\n",
        "                          kernel_size=(1, 5),\n",
        "                          stride=1,\n",
        "                          padding=(0, 2),\n",
        "                          bias=bias),\n",
        "                nn.BatchNorm2d(reduced_depth),\n",
        "                nn.PReLU()\n",
        "            )\n",
        "\n",
        "        else: # Regular Bottle Neck\n",
        "            self.ext_branch_2 = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=reduced_depth,\n",
        "                          out_channels=reduced_depth,\n",
        "                          kernel_size=3,\n",
        "                          stride=1,\n",
        "                          padding=1,\n",
        "                          bias=bias),\n",
        "                nn.BatchNorm2d(reduced_depth),\n",
        "                nn.PReLU()\n",
        "            )\n",
        "\n",
        "        self.ext_branch_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=reduced_depth,\n",
        "                      out_channels=output_ch,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(output_ch),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.regularizer = nn.Dropout2d(p=regularizer_prob)\n",
        "\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        main = x\n",
        "        ext = self.ext_branch_1(x)\n",
        "        ext = self.ext_branch_2(ext)\n",
        "        ext = self.ext_branch_3(ext)\n",
        "        ext = self.regularizer(ext)\n",
        "\n",
        "        out = self.prelu(main + ext)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DownSampleBottleNeck(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_ch,\n",
        "                 output_ch,\n",
        "                 projection_ratio=4,\n",
        "                 regularizer_prob=0,\n",
        "                 bias=False):\n",
        "        super(DownSampleBottleNeck, self).__init__()\n",
        "\n",
        "        reduced_depth = input_ch // projection_ratio\n",
        "\n",
        "        self.ext_branch_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_ch,\n",
        "                      out_channels=reduced_depth,\n",
        "                      kernel_size=2,\n",
        "                      stride=2,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(reduced_depth),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.ext_branch_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=reduced_depth,\n",
        "                      out_channels=reduced_depth,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(reduced_depth),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.ext_branch_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=reduced_depth,\n",
        "                      out_channels=output_ch,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(output_ch),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.regularizer = nn.Dropout2d(p=regularizer_prob)\n",
        "\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        main, ind = self.max_pool(x)\n",
        "        ext = self.ext_branch_1(x)\n",
        "        ext = self.ext_branch_2(ext)\n",
        "        ext = self.ext_branch_3(ext)\n",
        "        ext = self.regularizer(ext)\n",
        "\n",
        "        # Feature map padding\n",
        "        n, ch_ext, h, w = ext.size()\n",
        "        ch_main = main.size()[1]\n",
        "        padding = torch.autograd.Variable(torch.zeros(n, ch_ext-ch_main, h, w))\n",
        "        if main.is_cuda:\n",
        "            padding = padding.cuda()\n",
        "        main = torch.cat((main, padding), dim=1)\n",
        "\n",
        "        out = self.prelu(main + ext)\n",
        "\n",
        "        return out, ind\n",
        "\n",
        "\n",
        "class UpSampleBottleNeck(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_ch,\n",
        "                 output_ch,\n",
        "                 projection_ratio=4,\n",
        "                 regularizer_prob=0,\n",
        "                 bias=False):\n",
        "        super(UpSampleBottleNeck, self).__init__()\n",
        "\n",
        "        reduced_depth = input_ch // projection_ratio\n",
        "\n",
        "        self.main_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_ch,\n",
        "                      out_channels=output_ch,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(output_ch)\n",
        "        )\n",
        "\n",
        "        self.ext_branch_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_ch,\n",
        "                      out_channels=reduced_depth,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(reduced_depth),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.ext_branch_2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=reduced_depth,\n",
        "                      out_channels=reduced_depth,\n",
        "                      kernel_size=3,\n",
        "                      stride=2,\n",
        "                      padding=1,\n",
        "                      output_padding=1,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(reduced_depth),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.ext_branch_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=reduced_depth,\n",
        "                      out_channels=output_ch,\n",
        "                      kernel_size=1,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      bias=bias),\n",
        "            nn.BatchNorm2d(output_ch),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.regularizer = nn.Dropout2d(p=regularizer_prob)\n",
        "\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x, ind):\n",
        "        main = self.main_branch(x)\n",
        "        main = self.max_unpool(main, ind)\n",
        "        ext = self.ext_branch_1(x)\n",
        "        ext = self.ext_branch_2(ext)\n",
        "        ext = self.ext_branch_3(ext)\n",
        "        ext = self.regularizer(ext)\n",
        "\n",
        "        out = self.prelu(main + ext)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ENet(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(ENet, self).__init__()\n",
        "\n",
        "        # Initial\n",
        "        self.initial_block = InitialBlock(input_ch=input_ch, output_ch=16)\n",
        "\n",
        "        # Shared Encoder\n",
        "        # BottleNeck1\n",
        "        self.bottleNeck1_0 = DownSampleBottleNeck(input_ch=16, output_ch=64, regularizer_prob=0.01)\n",
        "        self.bottleNeck1_1 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.01)\n",
        "        self.bottleNeck1_2 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.01)\n",
        "        self.bottleNeck1_3 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.01)\n",
        "        self.bottleNeck1_4 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.01)\n",
        "\n",
        "        # BottleNeck2\n",
        "        self.bottleNeck2_0 = DownSampleBottleNeck(input_ch=64, output_ch=128, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_1 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_2 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=2, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_3 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_4 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=4, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_5 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_6 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=8, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_7 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.bottleNeck2_8 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=16, regularizer_prob=0.1)\n",
        "\n",
        "        # Binary Segmentation\n",
        "        # BottleNeck3\n",
        "        self.semBottleNeck3_0 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_1 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=2, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_2 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_3 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=4, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_4 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_5 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=8, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_6 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.semBottleNeck3_7 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=16, regularizer_prob=0.1)\n",
        "\n",
        "        # BottleNeck4\n",
        "        self.semBottleNeck4_0 = UpSampleBottleNeck(input_ch=128, output_ch=64, regularizer_prob=0.1)\n",
        "        self.semBottleNeck4_1 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.1)\n",
        "        self.semBottleNeck4_2 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.1)\n",
        "\n",
        "        # BottleNeck5\n",
        "        self.semBottleNeck5_0 = UpSampleBottleNeck(input_ch=64, output_ch=16, regularizer_prob=0.1)\n",
        "        self.semBottleNeck5_1 = RegularBottleNeck(input_ch=16, output_ch=16, regularizer_prob=0.1)\n",
        "\n",
        "        self.sem_out = nn.ConvTranspose2d(in_channels=16,\n",
        "                                          out_channels=output_ch,\n",
        "                                          kernel_size=3,\n",
        "                                          stride=2,\n",
        "                                          padding=1,\n",
        "                                          output_padding=1,\n",
        "                                          bias=False)\n",
        "\n",
        "        # Instance Segmentation\n",
        "        # BottleNeck3\n",
        "        self.insBottleNeck3_0 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_1 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=2, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_2 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_3 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=4, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_4 = RegularBottleNeck(input_ch=128, output_ch=128, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_5 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=8, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_6 = RegularBottleNeck(input_ch=128, output_ch=128, assymmetric=True, regularizer_prob=0.1)\n",
        "        self.insBottleNeck3_7 = RegularBottleNeck(input_ch=128, output_ch=128, dilation=16, regularizer_prob=0.1)\n",
        "\n",
        "        # BottleNeck4\n",
        "        self.insBottleNeck4_0 = UpSampleBottleNeck(input_ch=128, output_ch=64, regularizer_prob=0.1)\n",
        "        self.insBottleNeck4_1 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.1)\n",
        "        self.insBottleNeck4_2 = RegularBottleNeck(input_ch=64, output_ch=64, regularizer_prob=0.1)\n",
        "\n",
        "        # BottleNeck5\n",
        "        self.insBottleNeck5_0 = UpSampleBottleNeck(input_ch=64, output_ch=16, regularizer_prob=0.1)\n",
        "        self.insBottleNeck5_1 = RegularBottleNeck(input_ch=16, output_ch=16, regularizer_prob=0.1)\n",
        "\n",
        "        self.ins_out = nn.ConvTranspose2d(in_channels=16,\n",
        "                                          out_channels=5,\n",
        "                                          kernel_size=3,\n",
        "                                          stride=2,\n",
        "                                          padding=1,\n",
        "                                          output_padding=1,\n",
        "                                          bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial\n",
        "        x = self.initial_block(x)\n",
        "\n",
        "        # Shared Encoder\n",
        "        # Stage1\n",
        "        x, ind_1 = self.bottleNeck1_0(x)\n",
        "        x = self.bottleNeck1_1(x)\n",
        "        x = self.bottleNeck1_2(x)\n",
        "        x = self.bottleNeck1_3(x)\n",
        "        x = self.bottleNeck1_4(x)\n",
        "\n",
        "        # Stage2\n",
        "        x, ind_2 = self.bottleNeck2_0(x)\n",
        "        x = self.bottleNeck2_1(x)\n",
        "        x = self.bottleNeck2_2(x)\n",
        "        x = self.bottleNeck2_3(x)\n",
        "        x = self.bottleNeck2_4(x)\n",
        "        x = self.bottleNeck2_5(x)\n",
        "        x = self.bottleNeck2_6(x)\n",
        "        x = self.bottleNeck2_7(x)\n",
        "        x = self.bottleNeck2_8(x)\n",
        "\n",
        "        # Binary Segmentation\n",
        "        # Stage3\n",
        "        x1 = self.semBottleNeck3_0(x)\n",
        "        x1 = self.semBottleNeck3_1(x1)\n",
        "        x1 = self.semBottleNeck3_2(x1)\n",
        "        x1 = self.semBottleNeck3_3(x1)\n",
        "        x1 = self.semBottleNeck3_4(x1)\n",
        "        x1 = self.semBottleNeck3_5(x1)\n",
        "        x1 = self.semBottleNeck3_6(x1)\n",
        "        x1 = self.semBottleNeck3_7(x1)\n",
        "\n",
        "        # Stage4\n",
        "        x1 = self.semBottleNeck4_0(x1, ind_2)\n",
        "        x1 = self.semBottleNeck4_1(x1)\n",
        "        x1 = self.semBottleNeck4_2(x1)\n",
        "\n",
        "        # Stage5\n",
        "        x1 = self.semBottleNeck5_0(x1, ind_1)\n",
        "        x1 = self.semBottleNeck5_1(x1)\n",
        "\n",
        "        # Instance Segmentation\n",
        "        # Stage3\n",
        "        x2 = self.semBottleNeck3_0(x)\n",
        "        x2 = self.semBottleNeck3_1(x2)\n",
        "        x2 = self.semBottleNeck3_2(x2)\n",
        "        x2 = self.semBottleNeck3_3(x2)\n",
        "        x2 = self.semBottleNeck3_4(x2)\n",
        "        x2 = self.semBottleNeck3_5(x2)\n",
        "        x2 = self.semBottleNeck3_6(x2)\n",
        "        x2 = self.semBottleNeck3_7(x2)\n",
        "\n",
        "        # Stage4\n",
        "        x2 = self.semBottleNeck4_0(x2, ind_2)\n",
        "        x2 = self.semBottleNeck4_1(x2)\n",
        "        x2 = self.semBottleNeck4_2(x2)\n",
        "\n",
        "        # Stage5\n",
        "        x2 = self.semBottleNeck5_0(x2, ind_1)\n",
        "        x2 = self.semBottleNeck5_1(x2)\n",
        "\n",
        "        # Stage 6\n",
        "        sem = self.sem_out(x1)\n",
        "        ins = self.ins_out(x2)\n",
        "\n",
        "        return sem, ins"
      ],
      "metadata": {
        "id": "U_UIvURrjP0b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "CwolhkDy1Fut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS\n",
        "\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "class DiscriminativeLoss(_Loss):\n",
        "\n",
        "    def __init__(self, delta_var=0.5, delta_dist=1.5,\n",
        "                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
        "                 usegpu=True, size_average=True):\n",
        "        super(DiscriminativeLoss, self).__init__(size_average)\n",
        "        self.delta_var = delta_var\n",
        "        self.delta_dist = delta_dist\n",
        "        self.norm = norm\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.usegpu = usegpu\n",
        "        assert self.norm in [1, 2]\n",
        "\n",
        "    def forward(self, input, target, n_clusters):\n",
        "#         _assert_no_grad(target)\n",
        "        return self._discriminative_loss(input, target, n_clusters)\n",
        "\n",
        "    def _discriminative_loss(self, input, target, n_clusters):\n",
        "        bs, n_features, height, width = input.size()\n",
        "        max_n_clusters = target.size(1)\n",
        "\n",
        "        input = input.contiguous().view(bs, n_features, height * width)\n",
        "        target = target.contiguous().view(bs, max_n_clusters, height * width)\n",
        "\n",
        "        c_means = self._cluster_means(input, target, n_clusters)\n",
        "        l_var = self._variance_term(input, target, c_means, n_clusters)\n",
        "        l_dist = self._distance_term(c_means, n_clusters)\n",
        "        l_reg = self._regularization_term(c_means, n_clusters)\n",
        "\n",
        "        loss = self.alpha * l_var + self.beta * l_dist + self.gamma * l_reg\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _cluster_means(self, input, target, n_clusters):\n",
        "        bs, n_features, n_loc = input.size()\n",
        "        max_n_clusters = target.size(1)\n",
        "\n",
        "        # bs, n_features, max_n_clusters, n_loc\n",
        "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
        "        # bs, 1, max_n_clusters, n_loc\n",
        "        target = target.unsqueeze(1)\n",
        "        # bs, n_features, max_n_clusters, n_loc\n",
        "        input = input * target\n",
        "\n",
        "        means = []\n",
        "        for i in range(bs):\n",
        "            # n_features, n_clusters, n_loc\n",
        "            input_sample = input[i, :, :n_clusters[i]]\n",
        "            # 1, n_clusters, n_loc,\n",
        "            target_sample = target[i, :, :n_clusters[i]]\n",
        "            # n_features, n_cluster\n",
        "            mean_sample = input_sample.sum(2) / (target_sample.sum(2) + 0.00001)\n",
        "\n",
        "            # padding\n",
        "            n_pad_clusters = max_n_clusters - n_clusters[i]\n",
        "            assert n_pad_clusters >= 0\n",
        "            if n_pad_clusters > 0:\n",
        "                pad_sample = torch.zeros(n_features, n_pad_clusters)\n",
        "                pad_sample = Variable(pad_sample)\n",
        "                if self.usegpu:\n",
        "                    pad_sample = pad_sample.cuda()\n",
        "                mean_sample = torch.cat((mean_sample, pad_sample), dim=1)\n",
        "            means.append(mean_sample)\n",
        "\n",
        "        # bs, n_features, max_n_clusters\n",
        "        means = torch.stack(means)\n",
        "\n",
        "        return means\n",
        "\n",
        "    def _variance_term(self, input, target, c_means, n_clusters):\n",
        "        bs, n_features, n_loc = input.size()\n",
        "        max_n_clusters = target.size(1)\n",
        "\n",
        "        # bs, n_features, max_n_clusters, n_loc\n",
        "        c_means = c_means.unsqueeze(3).expand(bs, n_features, max_n_clusters, n_loc)\n",
        "        # bs, n_features, max_n_clusters, n_loc\n",
        "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
        "        # bs, max_n_clusters, n_loc\n",
        "        var = (torch.clamp(torch.norm((input - c_means), self.norm, 1) -\n",
        "                           self.delta_var, min=0) ** 2) * target\n",
        "\n",
        "        var_term = 0\n",
        "        for i in range(bs):\n",
        "            # n_clusters, n_loc\n",
        "            var_sample = var[i, :n_clusters[i]]\n",
        "            # n_clusters, n_loc\n",
        "            target_sample = target[i, :n_clusters[i]]\n",
        "\n",
        "            # n_clusters\n",
        "            c_var = var_sample.sum(1) / (target_sample.sum(1) + 0.00001)\n",
        "            var_term += c_var.sum() / int(n_clusters[i])\n",
        "        var_term /= bs\n",
        "\n",
        "        return var_term\n",
        "\n",
        "    def _distance_term(self, c_means, n_clusters):\n",
        "        bs, n_features, max_n_clusters = c_means.size()\n",
        "\n",
        "        dist_term = 0\n",
        "        for i in range(bs):\n",
        "            if n_clusters[i] <= 1:\n",
        "                continue\n",
        "\n",
        "            # n_features, n_clusters\n",
        "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
        "\n",
        "            # n_features, n_clusters, n_clusters\n",
        "            means_a = mean_sample.unsqueeze(2).expand(n_features, n_clusters[i], n_clusters[i])\n",
        "            means_b = means_a.permute(0, 2, 1)\n",
        "            diff = means_a - means_b\n",
        "\n",
        "            margin = 2 * self.delta_dist * (1.0 - torch.eye(n_clusters[i]))\n",
        "            margin = Variable(margin)\n",
        "            if self.usegpu:\n",
        "                margin = margin.cuda()\n",
        "            c_dist = torch.sum(torch.clamp(margin - torch.norm(diff, self.norm, 0), min=0) ** 2)\n",
        "            dist_term += c_dist / (2 * n_clusters[i] * (n_clusters[i] - 1))\n",
        "        dist_term /= bs\n",
        "\n",
        "        return dist_term\n",
        "\n",
        "    def _regularization_term(self, c_means, n_clusters):\n",
        "        bs, n_features, max_n_clusters = c_means.size()\n",
        "\n",
        "        reg_term = 0\n",
        "        for i in range(bs):\n",
        "            # n_features, n_clusters\n",
        "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
        "            reg_term += torch.mean(torch.norm(mean_sample, self.norm, 0))\n",
        "        reg_term /= bs\n",
        "\n",
        "        return reg_term"
      ],
      "metadata": {
        "id": "w3Es7sj1jfXa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "OSOmo6X81IZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from skimage.transform import AffineTransform, warp\n",
        "from skimage import img_as_float64, img_as_float32, img_as_ubyte\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "class tuSimpleDataset(data.Dataset):\n",
        "    # refer from : \n",
        "    # https://github.com/vxy10/ImageAugmentation\n",
        "    # https://github.com/TuSimple/tusimple-benchmark/blob/master/example/lane_demo.ipynb\n",
        "    def __init__(self, file_path, size=[640, 360], gray=True, train=True, intensity=10):\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "        self.width = size[0]\n",
        "        self.height = size[1]\n",
        "        self.n_seg = 5\n",
        "        self.file_path = file_path\n",
        "        self.flags = {'size':size, 'gray':gray, 'train':train, 'intensity':intensity}\n",
        "        self.json_lists = glob.glob(os.path.join(self.file_path, '*.json'))\n",
        "        self.labels = []\n",
        "        for json_list in self.json_lists:\n",
        "            self.labels += [json.loads(line) for line in open(json_list)]\n",
        "        self.lanes = [lane['lanes'] for lane in self.labels]\n",
        "        self.y_samples = [y_sample['h_samples'] for y_sample in self.labels]\n",
        "        self.raw_files = [raw_file['raw_file'] for raw_file in self.labels]\n",
        "\n",
        "        self.img = np.zeros(size, np.uint8)\n",
        "        self.label_img = np.zeros(size, np.uint8)\n",
        "        self.ins_img = np.zeros((0,size[0],size[1]), np.uint8)\n",
        "        \n",
        "        self.len = len(self.labels)\n",
        "        \n",
        "    def random_transform(self):\n",
        "        intensity=self.flags['intensity']\n",
        "        def _get_delta(intensity):\n",
        "            delta = np.radians(intensity)\n",
        "            rand_delta = np.random.uniform(low=-delta, high=delta)\n",
        "            return rand_delta\n",
        "\n",
        "        trans_M = AffineTransform(scale=(.9, .9),\n",
        "                                 translation=(-_get_delta(intensity), _get_delta(intensity)),\n",
        "                                 shear=_get_delta(intensity))\n",
        "        self.img = img_as_float32(self.img)\n",
        "        self.label_img = img_as_float32(self.label_img)\n",
        "        self.ins_img = img_as_float32(self.ins_img)\n",
        "\n",
        "        self.img = warp(self.img, trans_M)\n",
        "        self.label_img = warp(self.label_img, trans_M)\n",
        "        for i in range(len(self.ins_img)):\n",
        "            self.ins_img[i] = warp(self.ins_img[i], trans_M)\n",
        "    \n",
        "    def image_resize(self):\n",
        "        ins = []\n",
        "        self.img = cv2.resize(self.img, tuple(self.flags['size']), interpolation=cv2.INTER_CUBIC)\n",
        "        self.label_img = cv2.resize(self.label_img, tuple(self.flags['size']), interpolation=cv2.INTER_CUBIC)\n",
        "        for i in range(len(self.ins_img)):\n",
        "            dst = cv2.resize(self.ins_img[i], tuple(self.flags['size']), interpolation=cv2.INTER_CUBIC)\n",
        "            ins.append(dst)\n",
        "\n",
        "        self.ins_img = np.array(ins, dtype=np.uint8)\n",
        "    \n",
        "    def preprocess(self):\n",
        "        # CLAHE nomalization\n",
        "        img = cv2.cvtColor(self.img, cv2.COLOR_RGB2LAB)\n",
        "        img_plane = cv2.split(img)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        img_plane[0] = clahe.apply(img_plane[0])\n",
        "        img = cv2.merge(img_plane)\n",
        "        self.img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
        "   \n",
        "    def get_lane_image(self, idx):\n",
        "        lane_pts = [[(x,y) for (x,y) in zip(lane, self.y_samples[idx]) if x >= 0] for lane in self.lanes[idx]]\n",
        "        while len(lane_pts) < self.n_seg:\n",
        "            lane_pts.append(list())\n",
        "        self.img = plt.imread(os.path.join(self.file_path, self.raw_files[idx]))\n",
        "        self.height, self.width, _ = self.img.shape\n",
        "        self.label_img = np.zeros((self.height, self.width), dtype=np.uint8)\n",
        "        self.ins_img = np.zeros((0, self.height, self.width), dtype=np.uint8)\n",
        "        \n",
        "        for i, lane_pt in enumerate(lane_pts):\n",
        "            cv2.polylines(self.label_img, np.int32([lane_pt]), isClosed=False, color=(1), thickness=15)\n",
        "            gt = np.zeros((self.height, self.width), dtype=np.uint8)\n",
        "            gt = cv2.polylines(gt, np.int32([lane_pt]), isClosed=False, color=(1), thickness=7)\n",
        "            self.ins_img = np.concatenate([self.ins_img, gt[np.newaxis]])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.get_lane_image(idx)\n",
        "        self.image_resize()\n",
        "        self.preprocess()\n",
        "\n",
        "        if self.flags['train']:\n",
        "            #self.random_transform()\n",
        "            self.img = np.array(np.transpose(self.img, (2,0,1)), dtype=np.float32)\n",
        "            self.label_img = np.array(self.label_img, dtype=np.float32)\n",
        "            self.ins_img = np.array(self.ins_img, dtype=np.float32)\n",
        "            return torch.Tensor(self.img), torch.LongTensor(self.label_img), torch.Tensor(self.ins_img)\n",
        "        else:\n",
        "            self.img = np.array(np.transpose(self.img, (2,0,1)), dtype=np.float32)\n",
        "            return torch.Tensor(self.img)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ],
      "metadata": {
        "id": "TFEHvarSjjHq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logger"
      ],
      "metadata": {
        "id": "EsfRdu5R1K74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install scipy==1.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fo4H8qY-X29",
        "outputId": "860294d9-3ea2-4220-c82a-f6b527b9373f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGGER\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()"
      ],
      "metadata": {
        "id": "G0m0n8nvju81"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "JPiMLlur1NW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters for training"
      ],
      "metadata": {
        "id": "-UYrWjb4D-zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"train_path\": \"/content/tusimple/\",\n",
        "    \"lr\": 1e-5,\n",
        "    \"batch_size\": 10,\n",
        "    \"img_size\": [224, 224],\n",
        "    \"epoch\": 10\n",
        "}\n",
        "\n",
        "INPUT_CHANNELS = 3\n",
        "OUTPUT_CHANNELS = 2\n",
        "LEARNING_RATE = args[\"lr\"] #1e-5\n",
        "BATCH_SIZE = args[\"batch_size\"] #20\n",
        "NUM_EPOCHS = args[\"epoch\"] #100\n",
        "LOG_INTERVAL = 10\n",
        "SIZE = [args[\"img_size\"][0], args[\"img_size\"][1]] #[224, 224]"
      ],
      "metadata": {
        "id": "rmhIm2VIjzGl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manage DataSet and Drive connection"
      ],
      "metadata": {
        "id": "uh7UPj3-ECo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mound google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lGkwkHipcMdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4976e02-021f-4f33-e566-ce2a915c21ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the dataset\n",
        "\n",
        "from zipfile import ZipFile as TuSimpleCompressed\n",
        "file_name = \"/content/drive/MyDrive/tusimple.zip\"\n",
        "\n",
        "with TuSimpleCompressed(file_name, 'r') as tusimplerar:\n",
        "  tusimplerar.extractall()\n",
        "  print('Done')\n"
      ],
      "metadata": {
        "id": "3bol7rnY11fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a87c6a-5a44-4e4e-8d95-15ab7e15c328"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train method"
      ],
      "metadata": {
        "id": "gvkLwzSoEKvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # refer from : https://github.com/Sayan98/pytorch-segnet/blob/master/src/train.py\n",
        "    is_better = True\n",
        "    prev_loss = float('inf')\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_start = time.time()\n",
        "        loss_f = []\n",
        "\n",
        "        for batch_idx, (imgs, sem_labels, ins_labels) in enumerate(train_dataloader):\n",
        "            loss = 0\n",
        "\n",
        "            img_tensor = torch.autograd.Variable(imgs).cuda()\n",
        "            sem_tensor = torch.autograd.Variable(sem_labels).cuda()\n",
        "            ins_tensor = torch.autograd.Variable(ins_labels).cuda()\n",
        "\n",
        "            # Init gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Predictions\n",
        "            sem_pred, ins_pred = model(img_tensor)\n",
        "\n",
        "            # Discriminative Loss\n",
        "            disc_loss = criterion_disc(ins_pred, ins_tensor, [5] * len(img_tensor))\n",
        "            loss += disc_loss\n",
        "\n",
        "            # CrossEntropy Loss\n",
        "\n",
        "            ce_loss = criterion_ce(sem_pred.permute(0,2,3,1).contiguous().view(-1,OUTPUT_CHANNELS),\n",
        "                                   sem_tensor.view(-1))\n",
        "            loss += ce_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_f.append(loss.cpu().data.numpy())\n",
        "\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(imgs), len(train_dataloader.dataset),\n",
        "                    100. * batch_idx / len(train_dataloader), loss.item()))\n",
        "\n",
        "                #Tensorboard\n",
        "                info = {'loss': loss.item(), 'ce_loss': ce_loss.item(), 'disc_loss': disc_loss.item(), 'epoch': epoch}\n",
        "\n",
        "                for tag, value in info.items():\n",
        "                    logger.scalar_summary(tag, value, batch_idx + 1)\n",
        "\n",
        "                # 2. Log values and gradients of the parameters (histogram summary)\n",
        "                for tag, value in model.named_parameters():\n",
        "                    tag = tag.replace('.', '/')\n",
        "                    logger.histo_summary(tag, value.data.cpu().numpy(), batch_idx + 1)\n",
        "                    # logger.histo_summary(tag + '/grad', value.grad.data.cpu().numpy(), batch_idx + 1)\n",
        "\n",
        "                # 3. Log training images (image summary)\n",
        "                info = {'images': img_tensor.view(-1, 3, SIZE[0], SIZE[1])[:BATCH_SIZE].cpu().numpy(),\n",
        "                        'labels': sem_tensor.view(-1, SIZE[0], SIZE[1])[:BATCH_SIZE].cpu().numpy(),\n",
        "                        'sem_preds': sem_pred.view(-1, 2, SIZE[0], SIZE[1])[:BATCH_SIZE,1].data.cpu().numpy(),\n",
        "                        'ins_preds': ins_pred.view(-1, SIZE[0], SIZE[1])[:BATCH_SIZE*5].data.cpu().numpy()}\n",
        "\n",
        "                for tag, images in info.items():\n",
        "                    logger.image_summary(tag, images, batch_idx + 1)\n",
        "            \n",
        "        dt = time.time() - t_start\n",
        "        is_better = np.mean(loss_f) < prev_loss\n",
        "        scheduler.step()\n",
        "        \n",
        "        if is_better:\n",
        "            prev_loss = np.mean(loss_f)\n",
        "            print(\"\\t\\tBest Model.\")\n",
        "            torch.save(model.state_dict(), \"model_best.pth\")\n",
        "            \n",
        "        print(\"Epoch #{}\\tLoss: {:.8f}\\t Time: {:2f}s, Lr: {:2f}\".format(epoch+1, np.mean(loss_f), dt, optimizer.param_groups[0]['lr']))"
      ],
      "metadata": {
        "id": "wOuxL78vcf91"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup for training"
      ],
      "metadata": {
        "id": "D9ZenQ9wEQq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = Logger('./logs')\n",
        "\n",
        "train_path = args[\"train_path\"]\n",
        "train_dataset = tuSimpleDataset(train_path, size=SIZE)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16)"
      ],
      "metadata": {
        "id": "bBUzqYV7cjQ2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SegNet(input_ch=INPUT_CHANNELS, output_ch=OUTPUT_CHANNELS).cuda() \n",
        "#model = ENet(input_ch=INPUT_CHANNELS, output_ch=OUTPUT_CHANNELS).cuda() \n",
        "if os.path.isfile(\"model_best.pth\"):\n",
        "    print(\"Loaded model_best.pth\")\n",
        "    model.load_state_dict(torch.load(\"model_best.pth\"))\n",
        "\n",
        "criterion_ce = torch.nn.CrossEntropyLoss().cuda()\n",
        "criterion_disc = DiscriminativeLoss(delta_var=0.1,\n",
        "                                    delta_dist=0.6,\n",
        "                                    norm=2,\n",
        "                                    usegpu=True).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,30,40,50,60,70,80], gamma=0.9)"
      ],
      "metadata": {
        "id": "hUJIAqQHdhkm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training"
      ],
      "metadata": {
        "id": "jb_o6QVvEUEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngx0choy3nea",
        "outputId": "b69ff996-d773-43fd-a928-28ddfcba5389"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch: 0 [0/3626 (0%)]\tLoss: 0.052791\n",
            "\tTrain Epoch: 0 [100/3626 (3%)]\tLoss: 0.084385\n",
            "\tTrain Epoch: 0 [200/3626 (6%)]\tLoss: 0.039450\n",
            "\tTrain Epoch: 0 [300/3626 (8%)]\tLoss: 0.038566\n",
            "\tTrain Epoch: 0 [400/3626 (11%)]\tLoss: 0.063701\n",
            "\tTrain Epoch: 0 [500/3626 (14%)]\tLoss: 0.045661\n",
            "\tTrain Epoch: 0 [600/3626 (17%)]\tLoss: 0.080003\n",
            "\tTrain Epoch: 0 [700/3626 (19%)]\tLoss: 0.052896\n",
            "\tTrain Epoch: 0 [800/3626 (22%)]\tLoss: 0.037213\n",
            "\tTrain Epoch: 0 [900/3626 (25%)]\tLoss: 0.052984\n",
            "\tTrain Epoch: 0 [1000/3626 (28%)]\tLoss: 0.056744\n",
            "\tTrain Epoch: 0 [1100/3626 (30%)]\tLoss: 0.041646\n",
            "\tTrain Epoch: 0 [1200/3626 (33%)]\tLoss: 0.049170\n",
            "\tTrain Epoch: 0 [1300/3626 (36%)]\tLoss: 0.047215\n",
            "\tTrain Epoch: 0 [1400/3626 (39%)]\tLoss: 0.041723\n",
            "\tTrain Epoch: 0 [1500/3626 (41%)]\tLoss: 0.041390\n",
            "\tTrain Epoch: 0 [1600/3626 (44%)]\tLoss: 0.039931\n",
            "\tTrain Epoch: 0 [1700/3626 (47%)]\tLoss: 0.034243\n",
            "\tTrain Epoch: 0 [1800/3626 (50%)]\tLoss: 0.048249\n",
            "\tTrain Epoch: 0 [1900/3626 (52%)]\tLoss: 0.062499\n",
            "\tTrain Epoch: 0 [2000/3626 (55%)]\tLoss: 0.058259\n",
            "\tTrain Epoch: 0 [2100/3626 (58%)]\tLoss: 0.050492\n",
            "\tTrain Epoch: 0 [2200/3626 (61%)]\tLoss: 0.041678\n",
            "\tTrain Epoch: 0 [2300/3626 (63%)]\tLoss: 0.047353\n",
            "\tTrain Epoch: 0 [2400/3626 (66%)]\tLoss: 0.048799\n",
            "\tTrain Epoch: 0 [2500/3626 (69%)]\tLoss: 0.051885\n",
            "\tTrain Epoch: 0 [2600/3626 (72%)]\tLoss: 0.038288\n",
            "\tTrain Epoch: 0 [2700/3626 (74%)]\tLoss: 0.048267\n",
            "\tTrain Epoch: 0 [2800/3626 (77%)]\tLoss: 0.034336\n",
            "\tTrain Epoch: 0 [2900/3626 (80%)]\tLoss: 0.052320\n",
            "\tTrain Epoch: 0 [3000/3626 (83%)]\tLoss: 0.059020\n",
            "\tTrain Epoch: 0 [3100/3626 (85%)]\tLoss: 0.046129\n",
            "\tTrain Epoch: 0 [3200/3626 (88%)]\tLoss: 0.051034\n",
            "\tTrain Epoch: 0 [3300/3626 (91%)]\tLoss: 0.041435\n",
            "\tTrain Epoch: 0 [3400/3626 (94%)]\tLoss: 0.062568\n",
            "\tTrain Epoch: 0 [3500/3626 (96%)]\tLoss: 0.048830\n",
            "\tTrain Epoch: 0 [3600/3626 (99%)]\tLoss: 0.046052\n",
            "\t\tBest Model.\n",
            "Epoch #1\tLoss: 0.05057811\t Time: 380.017411s, Lr: 0.000010\n",
            "\tTrain Epoch: 1 [0/3626 (0%)]\tLoss: 0.046222\n",
            "\tTrain Epoch: 1 [100/3626 (3%)]\tLoss: 0.032377\n",
            "\tTrain Epoch: 1 [200/3626 (6%)]\tLoss: 0.051414\n",
            "\tTrain Epoch: 1 [300/3626 (8%)]\tLoss: 0.056828\n",
            "\tTrain Epoch: 1 [400/3626 (11%)]\tLoss: 0.049969\n",
            "\tTrain Epoch: 1 [500/3626 (14%)]\tLoss: 0.057002\n",
            "\tTrain Epoch: 1 [600/3626 (17%)]\tLoss: 0.065160\n",
            "\tTrain Epoch: 1 [700/3626 (19%)]\tLoss: 0.052190\n",
            "\tTrain Epoch: 1 [800/3626 (22%)]\tLoss: 0.045391\n",
            "\tTrain Epoch: 1 [900/3626 (25%)]\tLoss: 0.066139\n",
            "\tTrain Epoch: 1 [1000/3626 (28%)]\tLoss: 0.036768\n",
            "\tTrain Epoch: 1 [1100/3626 (30%)]\tLoss: 0.049636\n",
            "\tTrain Epoch: 1 [1200/3626 (33%)]\tLoss: 0.050044\n",
            "\tTrain Epoch: 1 [1300/3626 (36%)]\tLoss: 0.043107\n",
            "\tTrain Epoch: 1 [1400/3626 (39%)]\tLoss: 0.049191\n",
            "\tTrain Epoch: 1 [1500/3626 (41%)]\tLoss: 0.060623\n",
            "\tTrain Epoch: 1 [1600/3626 (44%)]\tLoss: 0.040025\n",
            "\tTrain Epoch: 1 [1700/3626 (47%)]\tLoss: 0.039530\n",
            "\tTrain Epoch: 1 [1800/3626 (50%)]\tLoss: 0.041331\n",
            "\tTrain Epoch: 1 [1900/3626 (52%)]\tLoss: 0.050309\n",
            "\tTrain Epoch: 1 [2000/3626 (55%)]\tLoss: 0.036285\n",
            "\tTrain Epoch: 1 [2100/3626 (58%)]\tLoss: 0.037247\n",
            "\tTrain Epoch: 1 [2200/3626 (61%)]\tLoss: 0.044418\n",
            "\tTrain Epoch: 1 [2300/3626 (63%)]\tLoss: 0.044620\n",
            "\tTrain Epoch: 1 [2400/3626 (66%)]\tLoss: 0.043338\n",
            "\tTrain Epoch: 1 [2500/3626 (69%)]\tLoss: 0.042383\n",
            "\tTrain Epoch: 1 [2600/3626 (72%)]\tLoss: 0.050825\n",
            "\tTrain Epoch: 1 [2700/3626 (74%)]\tLoss: 0.049121\n",
            "\tTrain Epoch: 1 [2800/3626 (77%)]\tLoss: 0.054951\n",
            "\tTrain Epoch: 1 [2900/3626 (80%)]\tLoss: 0.049671\n",
            "\tTrain Epoch: 1 [3000/3626 (83%)]\tLoss: 0.041664\n",
            "\tTrain Epoch: 1 [3100/3626 (85%)]\tLoss: 0.055548\n",
            "\tTrain Epoch: 1 [3200/3626 (88%)]\tLoss: 0.048547\n",
            "\tTrain Epoch: 1 [3300/3626 (91%)]\tLoss: 0.040291\n",
            "\tTrain Epoch: 1 [3400/3626 (94%)]\tLoss: 0.041800\n",
            "\tTrain Epoch: 1 [3500/3626 (96%)]\tLoss: 0.037773\n",
            "\tTrain Epoch: 1 [3600/3626 (99%)]\tLoss: 0.055150\n",
            "\t\tBest Model.\n",
            "Epoch #2\tLoss: 0.04951959\t Time: 379.367138s, Lr: 0.000010\n",
            "\tTrain Epoch: 2 [0/3626 (0%)]\tLoss: 0.057215\n",
            "\tTrain Epoch: 2 [100/3626 (3%)]\tLoss: 0.053516\n",
            "\tTrain Epoch: 2 [200/3626 (6%)]\tLoss: 0.034886\n",
            "\tTrain Epoch: 2 [300/3626 (8%)]\tLoss: 0.035722\n",
            "\tTrain Epoch: 2 [400/3626 (11%)]\tLoss: 0.045594\n",
            "\tTrain Epoch: 2 [500/3626 (14%)]\tLoss: 0.043511\n",
            "\tTrain Epoch: 2 [600/3626 (17%)]\tLoss: 0.041702\n",
            "\tTrain Epoch: 2 [700/3626 (19%)]\tLoss: 0.054807\n",
            "\tTrain Epoch: 2 [800/3626 (22%)]\tLoss: 0.054151\n",
            "\tTrain Epoch: 2 [900/3626 (25%)]\tLoss: 0.057261\n",
            "\tTrain Epoch: 2 [1000/3626 (28%)]\tLoss: 0.043211\n",
            "\tTrain Epoch: 2 [1100/3626 (30%)]\tLoss: 0.035443\n",
            "\tTrain Epoch: 2 [1200/3626 (33%)]\tLoss: 0.041067\n",
            "\tTrain Epoch: 2 [1300/3626 (36%)]\tLoss: 0.050541\n",
            "\tTrain Epoch: 2 [1400/3626 (39%)]\tLoss: 0.037433\n",
            "\tTrain Epoch: 2 [1500/3626 (41%)]\tLoss: 0.061720\n",
            "\tTrain Epoch: 2 [1600/3626 (44%)]\tLoss: 0.051835\n",
            "\tTrain Epoch: 2 [1700/3626 (47%)]\tLoss: 0.042411\n",
            "\tTrain Epoch: 2 [1800/3626 (50%)]\tLoss: 0.046742\n",
            "\tTrain Epoch: 2 [1900/3626 (52%)]\tLoss: 0.055459\n",
            "\tTrain Epoch: 2 [2000/3626 (55%)]\tLoss: 0.048871\n",
            "\tTrain Epoch: 2 [2100/3626 (58%)]\tLoss: 0.057947\n",
            "\tTrain Epoch: 2 [2200/3626 (61%)]\tLoss: 0.052632\n",
            "\tTrain Epoch: 2 [2300/3626 (63%)]\tLoss: 0.038021\n",
            "\tTrain Epoch: 2 [2400/3626 (66%)]\tLoss: 0.051029\n",
            "\tTrain Epoch: 2 [2500/3626 (69%)]\tLoss: 0.056882\n",
            "\tTrain Epoch: 2 [2600/3626 (72%)]\tLoss: 0.041039\n",
            "\tTrain Epoch: 2 [2700/3626 (74%)]\tLoss: 0.049128\n",
            "\tTrain Epoch: 2 [2800/3626 (77%)]\tLoss: 0.064527\n",
            "\tTrain Epoch: 2 [2900/3626 (80%)]\tLoss: 0.044115\n",
            "\tTrain Epoch: 2 [3000/3626 (83%)]\tLoss: 0.046340\n",
            "\tTrain Epoch: 2 [3100/3626 (85%)]\tLoss: 0.041523\n",
            "\tTrain Epoch: 2 [3200/3626 (88%)]\tLoss: 0.055125\n",
            "\tTrain Epoch: 2 [3300/3626 (91%)]\tLoss: 0.039455\n",
            "\tTrain Epoch: 2 [3400/3626 (94%)]\tLoss: 0.077392\n",
            "\tTrain Epoch: 2 [3500/3626 (96%)]\tLoss: 0.059903\n",
            "\tTrain Epoch: 2 [3600/3626 (99%)]\tLoss: 0.050913\n",
            "\t\tBest Model.\n",
            "Epoch #3\tLoss: 0.04806189\t Time: 379.197200s, Lr: 0.000010\n",
            "\tTrain Epoch: 3 [0/3626 (0%)]\tLoss: 0.054406\n",
            "\tTrain Epoch: 3 [100/3626 (3%)]\tLoss: 0.048091\n",
            "\tTrain Epoch: 3 [200/3626 (6%)]\tLoss: 0.044908\n",
            "\tTrain Epoch: 3 [300/3626 (8%)]\tLoss: 0.063512\n",
            "\tTrain Epoch: 3 [400/3626 (11%)]\tLoss: 0.056823\n",
            "\tTrain Epoch: 3 [500/3626 (14%)]\tLoss: 0.053774\n",
            "\tTrain Epoch: 3 [600/3626 (17%)]\tLoss: 0.046816\n",
            "\tTrain Epoch: 3 [700/3626 (19%)]\tLoss: 0.030126\n",
            "\tTrain Epoch: 3 [800/3626 (22%)]\tLoss: 0.057331\n",
            "\tTrain Epoch: 3 [900/3626 (25%)]\tLoss: 0.054235\n",
            "\tTrain Epoch: 3 [1000/3626 (28%)]\tLoss: 0.026418\n",
            "\tTrain Epoch: 3 [1100/3626 (30%)]\tLoss: 0.035146\n",
            "\tTrain Epoch: 3 [1200/3626 (33%)]\tLoss: 0.027968\n",
            "\tTrain Epoch: 3 [1300/3626 (36%)]\tLoss: 0.054664\n",
            "\tTrain Epoch: 3 [1400/3626 (39%)]\tLoss: 0.034606\n",
            "\tTrain Epoch: 3 [1500/3626 (41%)]\tLoss: 0.062687\n",
            "\tTrain Epoch: 3 [1600/3626 (44%)]\tLoss: 0.036986\n",
            "\tTrain Epoch: 3 [1700/3626 (47%)]\tLoss: 0.047559\n",
            "\tTrain Epoch: 3 [1800/3626 (50%)]\tLoss: 0.082913\n",
            "\tTrain Epoch: 3 [1900/3626 (52%)]\tLoss: 0.040136\n",
            "\tTrain Epoch: 3 [2000/3626 (55%)]\tLoss: 0.052655\n",
            "\tTrain Epoch: 3 [2100/3626 (58%)]\tLoss: 0.058565\n",
            "\tTrain Epoch: 3 [2200/3626 (61%)]\tLoss: 0.041195\n",
            "\tTrain Epoch: 3 [2300/3626 (63%)]\tLoss: 0.048870\n",
            "\tTrain Epoch: 3 [2400/3626 (66%)]\tLoss: 0.045774\n",
            "\tTrain Epoch: 3 [2500/3626 (69%)]\tLoss: 0.065815\n",
            "\tTrain Epoch: 3 [2600/3626 (72%)]\tLoss: 0.048191\n",
            "\tTrain Epoch: 3 [2700/3626 (74%)]\tLoss: 0.043135\n",
            "\tTrain Epoch: 3 [2800/3626 (77%)]\tLoss: 0.043438\n",
            "\tTrain Epoch: 3 [2900/3626 (80%)]\tLoss: 0.058770\n",
            "\tTrain Epoch: 3 [3000/3626 (83%)]\tLoss: 0.034631\n",
            "\tTrain Epoch: 3 [3100/3626 (85%)]\tLoss: 0.037345\n",
            "\tTrain Epoch: 3 [3200/3626 (88%)]\tLoss: 0.033414\n",
            "\tTrain Epoch: 3 [3300/3626 (91%)]\tLoss: 0.036074\n",
            "\tTrain Epoch: 3 [3400/3626 (94%)]\tLoss: 0.060519\n",
            "\tTrain Epoch: 3 [3500/3626 (96%)]\tLoss: 0.034929\n",
            "\tTrain Epoch: 3 [3600/3626 (99%)]\tLoss: 0.044058\n",
            "\t\tBest Model.\n",
            "Epoch #4\tLoss: 0.04709388\t Time: 377.008153s, Lr: 0.000009\n",
            "\tTrain Epoch: 4 [0/3626 (0%)]\tLoss: 0.053778\n",
            "\tTrain Epoch: 4 [100/3626 (3%)]\tLoss: 0.028461\n",
            "\tTrain Epoch: 4 [200/3626 (6%)]\tLoss: 0.048793\n",
            "\tTrain Epoch: 4 [300/3626 (8%)]\tLoss: 0.038694\n",
            "\tTrain Epoch: 4 [400/3626 (11%)]\tLoss: 0.052476\n",
            "\tTrain Epoch: 4 [500/3626 (14%)]\tLoss: 0.058998\n",
            "\tTrain Epoch: 4 [600/3626 (17%)]\tLoss: 0.048651\n",
            "\tTrain Epoch: 4 [700/3626 (19%)]\tLoss: 0.053613\n",
            "\tTrain Epoch: 4 [800/3626 (22%)]\tLoss: 0.044361\n",
            "\tTrain Epoch: 4 [900/3626 (25%)]\tLoss: 0.035318\n",
            "\tTrain Epoch: 4 [1000/3626 (28%)]\tLoss: 0.043219\n",
            "\tTrain Epoch: 4 [1100/3626 (30%)]\tLoss: 0.076896\n",
            "\tTrain Epoch: 4 [1200/3626 (33%)]\tLoss: 0.050450\n",
            "\tTrain Epoch: 4 [1300/3626 (36%)]\tLoss: 0.053159\n",
            "\tTrain Epoch: 4 [1400/3626 (39%)]\tLoss: 0.046625\n",
            "\tTrain Epoch: 4 [1500/3626 (41%)]\tLoss: 0.039031\n",
            "\tTrain Epoch: 4 [1600/3626 (44%)]\tLoss: 0.034575\n",
            "\tTrain Epoch: 4 [1700/3626 (47%)]\tLoss: 0.036996\n",
            "\tTrain Epoch: 4 [1800/3626 (50%)]\tLoss: 0.045200\n",
            "\tTrain Epoch: 4 [1900/3626 (52%)]\tLoss: 0.046747\n",
            "\tTrain Epoch: 4 [2000/3626 (55%)]\tLoss: 0.060231\n",
            "\tTrain Epoch: 4 [2100/3626 (58%)]\tLoss: 0.047385\n",
            "\tTrain Epoch: 4 [2200/3626 (61%)]\tLoss: 0.051009\n",
            "\tTrain Epoch: 4 [2300/3626 (63%)]\tLoss: 0.033649\n",
            "\tTrain Epoch: 4 [2400/3626 (66%)]\tLoss: 0.034434\n",
            "\tTrain Epoch: 4 [2500/3626 (69%)]\tLoss: 0.028130\n",
            "\tTrain Epoch: 4 [2600/3626 (72%)]\tLoss: 0.036964\n",
            "\tTrain Epoch: 4 [2700/3626 (74%)]\tLoss: 0.042952\n",
            "\tTrain Epoch: 4 [2800/3626 (77%)]\tLoss: 0.047492\n",
            "\tTrain Epoch: 4 [2900/3626 (80%)]\tLoss: 0.047818\n",
            "\tTrain Epoch: 4 [3000/3626 (83%)]\tLoss: 0.044108\n",
            "\tTrain Epoch: 4 [3100/3626 (85%)]\tLoss: 0.059763\n",
            "\tTrain Epoch: 4 [3200/3626 (88%)]\tLoss: 0.053168\n",
            "\tTrain Epoch: 4 [3300/3626 (91%)]\tLoss: 0.052859\n",
            "\tTrain Epoch: 4 [3400/3626 (94%)]\tLoss: 0.048105\n",
            "\tTrain Epoch: 4 [3500/3626 (96%)]\tLoss: 0.045857\n",
            "\tTrain Epoch: 4 [3600/3626 (99%)]\tLoss: 0.030745\n",
            "\t\tBest Model.\n",
            "Epoch #5\tLoss: 0.04551448\t Time: 376.666710s, Lr: 0.000009\n",
            "\tTrain Epoch: 5 [0/3626 (0%)]\tLoss: 0.039987\n",
            "\tTrain Epoch: 5 [100/3626 (3%)]\tLoss: 0.045509\n",
            "\tTrain Epoch: 5 [200/3626 (6%)]\tLoss: 0.066353\n",
            "\tTrain Epoch: 5 [300/3626 (8%)]\tLoss: 0.040468\n",
            "\tTrain Epoch: 5 [400/3626 (11%)]\tLoss: 0.038310\n",
            "\tTrain Epoch: 5 [500/3626 (14%)]\tLoss: 0.051441\n",
            "\tTrain Epoch: 5 [600/3626 (17%)]\tLoss: 0.039738\n",
            "\tTrain Epoch: 5 [700/3626 (19%)]\tLoss: 0.040244\n",
            "\tTrain Epoch: 5 [800/3626 (22%)]\tLoss: 0.038905\n",
            "\tTrain Epoch: 5 [900/3626 (25%)]\tLoss: 0.054391\n",
            "\tTrain Epoch: 5 [1000/3626 (28%)]\tLoss: 0.044859\n",
            "\tTrain Epoch: 5 [1100/3626 (30%)]\tLoss: 0.048972\n",
            "\tTrain Epoch: 5 [1200/3626 (33%)]\tLoss: 0.062741\n",
            "\tTrain Epoch: 5 [1300/3626 (36%)]\tLoss: 0.068579\n",
            "\tTrain Epoch: 5 [1400/3626 (39%)]\tLoss: 0.047027\n",
            "\tTrain Epoch: 5 [1500/3626 (41%)]\tLoss: 0.045830\n",
            "\tTrain Epoch: 5 [1600/3626 (44%)]\tLoss: 0.040507\n",
            "\tTrain Epoch: 5 [1700/3626 (47%)]\tLoss: 0.037244\n",
            "\tTrain Epoch: 5 [1800/3626 (50%)]\tLoss: 0.060128\n",
            "\tTrain Epoch: 5 [1900/3626 (52%)]\tLoss: 0.065236\n",
            "\tTrain Epoch: 5 [2000/3626 (55%)]\tLoss: 0.040124\n",
            "\tTrain Epoch: 5 [2100/3626 (58%)]\tLoss: 0.054986\n",
            "\tTrain Epoch: 5 [2200/3626 (61%)]\tLoss: 0.028613\n",
            "\tTrain Epoch: 5 [2300/3626 (63%)]\tLoss: 0.050102\n",
            "\tTrain Epoch: 5 [2400/3626 (66%)]\tLoss: 0.046481\n",
            "\tTrain Epoch: 5 [2500/3626 (69%)]\tLoss: 0.048563\n",
            "\tTrain Epoch: 5 [2600/3626 (72%)]\tLoss: 0.045822\n",
            "\tTrain Epoch: 5 [2700/3626 (74%)]\tLoss: 0.056728\n",
            "\tTrain Epoch: 5 [2800/3626 (77%)]\tLoss: 0.045902\n",
            "\tTrain Epoch: 5 [2900/3626 (80%)]\tLoss: 0.046757\n",
            "\tTrain Epoch: 5 [3000/3626 (83%)]\tLoss: 0.043625\n",
            "\tTrain Epoch: 5 [3100/3626 (85%)]\tLoss: 0.041556\n",
            "\tTrain Epoch: 5 [3200/3626 (88%)]\tLoss: 0.040416\n",
            "\tTrain Epoch: 5 [3300/3626 (91%)]\tLoss: 0.045401\n",
            "\tTrain Epoch: 5 [3400/3626 (94%)]\tLoss: 0.043489\n",
            "\tTrain Epoch: 5 [3500/3626 (96%)]\tLoss: 0.038503\n",
            "\tTrain Epoch: 5 [3600/3626 (99%)]\tLoss: 0.038378\n",
            "\t\tBest Model.\n",
            "Epoch #6\tLoss: 0.04458997\t Time: 377.541611s, Lr: 0.000009\n",
            "\tTrain Epoch: 6 [0/3626 (0%)]\tLoss: 0.024574\n",
            "\tTrain Epoch: 6 [100/3626 (3%)]\tLoss: 0.047764\n",
            "\tTrain Epoch: 6 [200/3626 (6%)]\tLoss: 0.031571\n",
            "\tTrain Epoch: 6 [300/3626 (8%)]\tLoss: 0.064615\n",
            "\tTrain Epoch: 6 [400/3626 (11%)]\tLoss: 0.033312\n",
            "\tTrain Epoch: 6 [500/3626 (14%)]\tLoss: 0.073776\n",
            "\tTrain Epoch: 6 [600/3626 (17%)]\tLoss: 0.045743\n",
            "\tTrain Epoch: 6 [700/3626 (19%)]\tLoss: 0.043310\n",
            "\tTrain Epoch: 6 [800/3626 (22%)]\tLoss: 0.051401\n",
            "\tTrain Epoch: 6 [900/3626 (25%)]\tLoss: 0.058456\n",
            "\tTrain Epoch: 6 [1000/3626 (28%)]\tLoss: 0.039866\n",
            "\tTrain Epoch: 6 [1100/3626 (30%)]\tLoss: 0.027909\n",
            "\tTrain Epoch: 6 [1200/3626 (33%)]\tLoss: 0.040418\n",
            "\tTrain Epoch: 6 [1300/3626 (36%)]\tLoss: 0.040340\n",
            "\tTrain Epoch: 6 [1400/3626 (39%)]\tLoss: 0.033438\n",
            "\tTrain Epoch: 6 [1500/3626 (41%)]\tLoss: 0.045767\n",
            "\tTrain Epoch: 6 [1600/3626 (44%)]\tLoss: 0.047274\n",
            "\tTrain Epoch: 6 [1700/3626 (47%)]\tLoss: 0.053275\n",
            "\tTrain Epoch: 6 [1800/3626 (50%)]\tLoss: 0.055385\n",
            "\tTrain Epoch: 6 [1900/3626 (52%)]\tLoss: 0.046913\n",
            "\tTrain Epoch: 6 [2000/3626 (55%)]\tLoss: 0.047705\n",
            "\tTrain Epoch: 6 [2100/3626 (58%)]\tLoss: 0.039068\n",
            "\tTrain Epoch: 6 [2200/3626 (61%)]\tLoss: 0.040140\n",
            "\tTrain Epoch: 6 [2300/3626 (63%)]\tLoss: 0.035967\n",
            "\tTrain Epoch: 6 [2400/3626 (66%)]\tLoss: 0.039774\n",
            "\tTrain Epoch: 6 [2500/3626 (69%)]\tLoss: 0.051043\n",
            "\tTrain Epoch: 6 [2600/3626 (72%)]\tLoss: 0.046901\n",
            "\tTrain Epoch: 6 [2700/3626 (74%)]\tLoss: 0.043667\n",
            "\tTrain Epoch: 6 [2800/3626 (77%)]\tLoss: 0.057569\n",
            "\tTrain Epoch: 6 [2900/3626 (80%)]\tLoss: 0.038658\n",
            "\tTrain Epoch: 6 [3000/3626 (83%)]\tLoss: 0.061117\n",
            "\tTrain Epoch: 6 [3100/3626 (85%)]\tLoss: 0.052384\n",
            "\tTrain Epoch: 6 [3200/3626 (88%)]\tLoss: 0.051522\n",
            "\tTrain Epoch: 6 [3300/3626 (91%)]\tLoss: 0.047308\n",
            "\tTrain Epoch: 6 [3400/3626 (94%)]\tLoss: 0.046763\n",
            "\tTrain Epoch: 6 [3500/3626 (96%)]\tLoss: 0.039243\n",
            "\tTrain Epoch: 6 [3600/3626 (99%)]\tLoss: 0.041396\n",
            "\t\tBest Model.\n",
            "Epoch #7\tLoss: 0.04381244\t Time: 375.320836s, Lr: 0.000009\n",
            "\tTrain Epoch: 7 [0/3626 (0%)]\tLoss: 0.039486\n",
            "\tTrain Epoch: 7 [100/3626 (3%)]\tLoss: 0.033975\n",
            "\tTrain Epoch: 7 [200/3626 (6%)]\tLoss: 0.053230\n",
            "\tTrain Epoch: 7 [300/3626 (8%)]\tLoss: 0.024508\n",
            "\tTrain Epoch: 7 [400/3626 (11%)]\tLoss: 0.044082\n",
            "\tTrain Epoch: 7 [500/3626 (14%)]\tLoss: 0.025662\n",
            "\tTrain Epoch: 7 [600/3626 (17%)]\tLoss: 0.050396\n",
            "\tTrain Epoch: 7 [700/3626 (19%)]\tLoss: 0.028574\n",
            "\tTrain Epoch: 7 [800/3626 (22%)]\tLoss: 0.044521\n",
            "\tTrain Epoch: 7 [900/3626 (25%)]\tLoss: 0.035323\n",
            "\tTrain Epoch: 7 [1000/3626 (28%)]\tLoss: 0.038994\n",
            "\tTrain Epoch: 7 [1100/3626 (30%)]\tLoss: 0.044026\n",
            "\tTrain Epoch: 7 [1200/3626 (33%)]\tLoss: 0.040711\n",
            "\tTrain Epoch: 7 [1300/3626 (36%)]\tLoss: 0.056433\n",
            "\tTrain Epoch: 7 [1400/3626 (39%)]\tLoss: 0.039618\n",
            "\tTrain Epoch: 7 [1500/3626 (41%)]\tLoss: 0.044975\n",
            "\tTrain Epoch: 7 [1600/3626 (44%)]\tLoss: 0.026545\n",
            "\tTrain Epoch: 7 [1700/3626 (47%)]\tLoss: 0.037137\n",
            "\tTrain Epoch: 7 [1800/3626 (50%)]\tLoss: 0.056285\n",
            "\tTrain Epoch: 7 [1900/3626 (52%)]\tLoss: 0.047252\n",
            "\tTrain Epoch: 7 [2000/3626 (55%)]\tLoss: 0.035755\n",
            "\tTrain Epoch: 7 [2100/3626 (58%)]\tLoss: 0.046414\n",
            "\tTrain Epoch: 7 [2200/3626 (61%)]\tLoss: 0.038336\n",
            "\tTrain Epoch: 7 [2300/3626 (63%)]\tLoss: 0.065832\n",
            "\tTrain Epoch: 7 [2400/3626 (66%)]\tLoss: 0.037538\n",
            "\tTrain Epoch: 7 [2500/3626 (69%)]\tLoss: 0.045217\n",
            "\tTrain Epoch: 7 [2600/3626 (72%)]\tLoss: 0.037796\n",
            "\tTrain Epoch: 7 [2700/3626 (74%)]\tLoss: 0.052878\n",
            "\tTrain Epoch: 7 [2800/3626 (77%)]\tLoss: 0.043664\n",
            "\tTrain Epoch: 7 [2900/3626 (80%)]\tLoss: 0.038377\n",
            "\tTrain Epoch: 7 [3000/3626 (83%)]\tLoss: 0.045412\n",
            "\tTrain Epoch: 7 [3100/3626 (85%)]\tLoss: 0.031687\n",
            "\tTrain Epoch: 7 [3200/3626 (88%)]\tLoss: 0.037757\n",
            "\tTrain Epoch: 7 [3300/3626 (91%)]\tLoss: 0.042876\n",
            "\tTrain Epoch: 7 [3400/3626 (94%)]\tLoss: 0.050190\n",
            "\tTrain Epoch: 7 [3500/3626 (96%)]\tLoss: 0.049889\n",
            "\tTrain Epoch: 7 [3600/3626 (99%)]\tLoss: 0.031873\n",
            "\t\tBest Model.\n",
            "Epoch #8\tLoss: 0.04322505\t Time: 376.079275s, Lr: 0.000009\n",
            "\tTrain Epoch: 8 [0/3626 (0%)]\tLoss: 0.036820\n",
            "\tTrain Epoch: 8 [100/3626 (3%)]\tLoss: 0.036886\n",
            "\tTrain Epoch: 8 [200/3626 (6%)]\tLoss: 0.056870\n",
            "\tTrain Epoch: 8 [300/3626 (8%)]\tLoss: 0.042340\n",
            "\tTrain Epoch: 8 [400/3626 (11%)]\tLoss: 0.051370\n",
            "\tTrain Epoch: 8 [500/3626 (14%)]\tLoss: 0.054258\n",
            "\tTrain Epoch: 8 [600/3626 (17%)]\tLoss: 0.043968\n",
            "\tTrain Epoch: 8 [700/3626 (19%)]\tLoss: 0.043238\n",
            "\tTrain Epoch: 8 [800/3626 (22%)]\tLoss: 0.049308\n",
            "\tTrain Epoch: 8 [900/3626 (25%)]\tLoss: 0.053586\n",
            "\tTrain Epoch: 8 [1000/3626 (28%)]\tLoss: 0.032271\n",
            "\tTrain Epoch: 8 [1100/3626 (30%)]\tLoss: 0.055669\n",
            "\tTrain Epoch: 8 [1200/3626 (33%)]\tLoss: 0.031161\n",
            "\tTrain Epoch: 8 [1300/3626 (36%)]\tLoss: 0.050892\n",
            "\tTrain Epoch: 8 [1400/3626 (39%)]\tLoss: 0.046718\n",
            "\tTrain Epoch: 8 [1500/3626 (41%)]\tLoss: 0.063367\n",
            "\tTrain Epoch: 8 [1600/3626 (44%)]\tLoss: 0.048970\n",
            "\tTrain Epoch: 8 [1700/3626 (47%)]\tLoss: 0.043295\n",
            "\tTrain Epoch: 8 [1800/3626 (50%)]\tLoss: 0.058009\n",
            "\tTrain Epoch: 8 [1900/3626 (52%)]\tLoss: 0.056758\n",
            "\tTrain Epoch: 8 [2000/3626 (55%)]\tLoss: 0.039614\n",
            "\tTrain Epoch: 8 [2100/3626 (58%)]\tLoss: 0.046259\n",
            "\tTrain Epoch: 8 [2200/3626 (61%)]\tLoss: 0.030886\n",
            "\tTrain Epoch: 8 [2300/3626 (63%)]\tLoss: 0.036272\n",
            "\tTrain Epoch: 8 [2400/3626 (66%)]\tLoss: 0.030563\n",
            "\tTrain Epoch: 8 [2500/3626 (69%)]\tLoss: 0.046793\n",
            "\tTrain Epoch: 8 [2600/3626 (72%)]\tLoss: 0.032214\n",
            "\tTrain Epoch: 8 [2700/3626 (74%)]\tLoss: 0.040519\n",
            "\tTrain Epoch: 8 [2800/3626 (77%)]\tLoss: 0.041607\n",
            "\tTrain Epoch: 8 [2900/3626 (80%)]\tLoss: 0.057591\n",
            "\tTrain Epoch: 8 [3000/3626 (83%)]\tLoss: 0.049502\n",
            "\tTrain Epoch: 8 [3100/3626 (85%)]\tLoss: 0.031211\n",
            "\tTrain Epoch: 8 [3200/3626 (88%)]\tLoss: 0.038381\n",
            "\tTrain Epoch: 8 [3300/3626 (91%)]\tLoss: 0.031198\n",
            "\tTrain Epoch: 8 [3400/3626 (94%)]\tLoss: 0.046328\n",
            "\tTrain Epoch: 8 [3500/3626 (96%)]\tLoss: 0.060250\n",
            "\tTrain Epoch: 8 [3600/3626 (99%)]\tLoss: 0.039017\n",
            "\t\tBest Model.\n",
            "Epoch #9\tLoss: 0.04221848\t Time: 376.452669s, Lr: 0.000009\n",
            "\tTrain Epoch: 9 [0/3626 (0%)]\tLoss: 0.024922\n",
            "\tTrain Epoch: 9 [100/3626 (3%)]\tLoss: 0.036667\n",
            "\tTrain Epoch: 9 [200/3626 (6%)]\tLoss: 0.030441\n",
            "\tTrain Epoch: 9 [300/3626 (8%)]\tLoss: 0.052193\n",
            "\tTrain Epoch: 9 [400/3626 (11%)]\tLoss: 0.031513\n",
            "\tTrain Epoch: 9 [500/3626 (14%)]\tLoss: 0.036969\n",
            "\tTrain Epoch: 9 [600/3626 (17%)]\tLoss: 0.043119\n",
            "\tTrain Epoch: 9 [700/3626 (19%)]\tLoss: 0.032538\n",
            "\tTrain Epoch: 9 [800/3626 (22%)]\tLoss: 0.029753\n",
            "\tTrain Epoch: 9 [900/3626 (25%)]\tLoss: 0.029129\n",
            "\tTrain Epoch: 9 [1000/3626 (28%)]\tLoss: 0.031495\n",
            "\tTrain Epoch: 9 [1100/3626 (30%)]\tLoss: 0.037818\n",
            "\tTrain Epoch: 9 [1200/3626 (33%)]\tLoss: 0.036960\n",
            "\tTrain Epoch: 9 [1300/3626 (36%)]\tLoss: 0.054975\n",
            "\tTrain Epoch: 9 [1400/3626 (39%)]\tLoss: 0.056042\n",
            "\tTrain Epoch: 9 [1500/3626 (41%)]\tLoss: 0.060569\n",
            "\tTrain Epoch: 9 [1600/3626 (44%)]\tLoss: 0.062179\n",
            "\tTrain Epoch: 9 [1700/3626 (47%)]\tLoss: 0.052935\n",
            "\tTrain Epoch: 9 [1800/3626 (50%)]\tLoss: 0.040928\n",
            "\tTrain Epoch: 9 [1900/3626 (52%)]\tLoss: 0.045919\n",
            "\tTrain Epoch: 9 [2000/3626 (55%)]\tLoss: 0.057104\n",
            "\tTrain Epoch: 9 [2100/3626 (58%)]\tLoss: 0.050762\n",
            "\tTrain Epoch: 9 [2200/3626 (61%)]\tLoss: 0.037341\n",
            "\tTrain Epoch: 9 [2300/3626 (63%)]\tLoss: 0.046628\n",
            "\tTrain Epoch: 9 [2400/3626 (66%)]\tLoss: 0.043518\n",
            "\tTrain Epoch: 9 [2500/3626 (69%)]\tLoss: 0.056435\n",
            "\tTrain Epoch: 9 [2600/3626 (72%)]\tLoss: 0.053261\n",
            "\tTrain Epoch: 9 [2700/3626 (74%)]\tLoss: 0.035782\n",
            "\tTrain Epoch: 9 [2800/3626 (77%)]\tLoss: 0.039833\n",
            "\tTrain Epoch: 9 [2900/3626 (80%)]\tLoss: 0.056433\n",
            "\tTrain Epoch: 9 [3000/3626 (83%)]\tLoss: 0.064564\n",
            "\tTrain Epoch: 9 [3100/3626 (85%)]\tLoss: 0.030468\n",
            "\tTrain Epoch: 9 [3200/3626 (88%)]\tLoss: 0.048008\n",
            "\tTrain Epoch: 9 [3300/3626 (91%)]\tLoss: 0.052050\n",
            "\tTrain Epoch: 9 [3400/3626 (94%)]\tLoss: 0.046588\n",
            "\tTrain Epoch: 9 [3500/3626 (96%)]\tLoss: 0.042413\n",
            "\tTrain Epoch: 9 [3600/3626 (99%)]\tLoss: 0.043419\n",
            "\t\tBest Model.\n",
            "Epoch #10\tLoss: 0.04185854\t Time: 377.882461s, Lr: 0.000009\n"
          ]
        }
      ]
    }
  ]
}